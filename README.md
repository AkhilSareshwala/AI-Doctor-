# ğŸ©º AI Doctor â€” Clinical Reasoning LLM Fine-Tuning

---

## ğŸ“Œ Overview
AI Doctor is a fine-tuning workflow for a **DeepSeek-R1-Distill-Llama-8B** model, specialized in **clinical reasoning, diagnostics, and treatment planning**.  
It uses:
- **Unsloth** for optimized LLM training
- **LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning
- **HuggingFace datasets & trainer**
- **Prompt engineering** for medical reasoning
- **W&B** for experiment tracking

---

## ğŸ“œ Workflow Steps

### 1ï¸âƒ£ Environment Setup
- ğŸ“¦ Install **Unsloth**, HuggingFace libraries, and `wandb`.
- ğŸ”‘ Authenticate HuggingFace & Weights & Biases API tokens.
- âš¡ Verify GPU availability (CUDA).

### 2ï¸âƒ£ Model Initialization
- ğŸ¦™ Load **DeepSeek-R1-Distill-Llama-8B** model.
- âš™ï¸ Enable **4-bit quantization** for efficiency.
- ğŸ“ Set maximum sequence length to **2048 tokens**.

### 3ï¸âƒ£ Prompt Engineering
- ğŸ§  Create **expert medical reasoning prompts** with Chain-of-Thought (CoT) design.
- âœï¸ Format input-output examples for both inference and training.

### 4ï¸âƒ£ Pre-Fine-Tuning Inference
- ğŸ” Test baseline model responses using sample medical queries.
- ğŸ“Š Compare response quality before fine-tuning.

### 5ï¸âƒ£ Dataset Preparation
- ğŸ“‚ Load **FreedomIntelligence/medical-o1-reasoning-SFT** dataset.
- ğŸ¥ Include **stepwise CoT reasoning** in each training prompt.
- ğŸ”„ Map dataset into the expected text format.

### 6ï¸âƒ£ Fine-Tuning (LoRA)
- ğŸŸ¦ Apply **LoRA adapters** to attention & projection layers.
- ğŸ›  Configure training with:
  - Batch size: 2
  - Gradient accumulation: 4
  - Learning rate: 2e-4
  - Steps: 60
- ğŸ“ˆ Track training in Weights & Biases.

### 7ï¸âƒ£ Post-Fine-Tuning Testing
- ğŸ©º Run inference with the fine-tuned LoRA model.
- ğŸ“Œ Compare outputs to baseline for improved reasoning accuracy.

---

## âš™ï¸ Tech Stack
- **Programming Language:** Python
- **Framework:** PyTorch
- **Libraries:** Unsloth, HuggingFace Transformers, Datasets, TRL, WandB
- **Hardware:** NVIDIA T4 GPU (Colab)

---

## ğŸ“Š Results
âœ… Improved step-by-step reasoning in medical diagnosis.  
âœ… More accurate and context-aware treatment recommendations.  
âœ… Reduced hallucination through structured CoT prompts.

